# Automatically Recognising Speech

In this section, we‚Äôll take a look at how Transformers can be used to convert spoken speech into text, a task known _speech recognition_.

<figure>
<img src=https://github.com/sanchit-gandhi/notebooks/raw/main/asr_diagram.jpg alt="Trulli" style="width:100%">
</figure>

Speech recognition, also known as automatic speech recognition (ASR) or speech-to-text (STT), is one of the most popular and exciting spoken language processing tasks. It‚Äôs used in a wide range of applications, including dictation, voice assistants, video captioning and meeting transcriptions.

You‚Äôve probably made use of a speech recognition system many times before without realising! Consider the digital assistant in your smartphone device (Siri, Google Assistant, Alexa). When you use these assistants, the first thing that they do is transcribe your spoken speech to written text, ready to be used for any downstream tasks (such as finding you the weather üòâ).

Have a play with the speech recognition demo below. You can either record yourself using your microphone or drag and drop an audio sample for transcription.

<!--- TODO: not sure whether this is correct, test before deploying --->
<gradio-app space=sanchit-gandhi/whisper-small.en"></gradio-app>

Speech recognition is a challenging task as it requires joint knowledge of audio and text. The input audio might have lots of background noise and be spoken by speakers with different accents, making it difficult to pick-out the spoken speech. The written text might have characters which don‚Äôt have an acoustic sound, such as punctuation, which are difficult to infer from audio alone. These are all hurdles we have to tackle when building effective speech recognition systems.

Now that we‚Äôve defined our task, we can begin looking into speech recognition in more detail. Specifically, we‚Äôll cover:

* [How to choose a dataset](#how-to-choose-a-dataset)
* [How to load a dataset](#how-to-load-an-audio-dataset)
* [What models we can use](#models-for-speech-recognition)
* [How to prepare audio-text data](#preprocessing-the-data)
* [Metrics for speech recognition](#metrics-for-speech-recognition)
* [How to fine-tune an ASR system with the Trainer API](#fine-tuning-a-speech-recognition-system-with-the-trainer-api)

## How to choose a dataset
As with any machine learning problem, our model is only as good as the data that we train it on. Speech recognition datasets vary considerably in how they are curated and the domains that they cover. To pick the right dataset, we need to match our criteria with the features that a dataset offers.

Before we pick a dataset, we first need to understand some of the key defining features.

### 1. Number of hours
Simply put, the number of training hours indicates how large the dataset is. It‚Äôs analogous to the number of training examples in an NLP dataset. However, bigger datasets aren‚Äôt necessarily better. If we want a model that generalises well, we want a diverse dataset with lots of different speakers, domains and speaking styles.

### 2. Domain
The domain entails where the data was sourced from, whether it be audiobooks, podcasts, YouTube or financial meetings. Each domain has a different distribution of data. For example, audiobooks are recorded in high-quality studio conditions (with no background noise) and text that is taken from written literature. Whereas for YouTube, the audio likely contains more background noise and a more informal style of speech.

We need to match our domain to the conditions we anticipate at inference time. For instance, if we train our model on audiobooks, we can‚Äôt expect our model to perform well in noisy environments.

### 3. Speaking style
The speaking style falls into one of two categories:

* Narrated: read from a script

* Spontaneous: un-scripted, conversational speech

The audio and text data reflect the style of speaking. Since narrated text is scripted, it tends to be spoken articulately and without any errors:

‚ÄúConsider the task of training a model on a speech recognition dataset‚Äù

Whereas for spontaneous speech, we can expect a more colloquial style of speech, with the inclusion of repetitions, hesitations and false-starts:

‚ÄúLet‚Äôs <uhm> take a look at how <silence> you‚Äôd you‚Äôd go about training a model on <uhm> a sp- speech recognition dataset‚Äù (this is an extreme example)

### 4. Transcription style
The transcription style refers to whether the target text has punctuation, casing or both. If we want a system to generate fully formatted text that could be used for a publication or meeting transcription, we require training data with punctuation and casing. If we just require the spoken words in an unformatted structure, neither punctuation nor casing are necessary. In this case, we can either pick a dataset without punctuation or casing, or pick one that has punctuation and casing and then subsequently remove them from the target text through pre-processing.

### A summary of datasets on the Hub
Here is a summary of popular speech recognition datasets on the Hugging Face Hub:

|                                                                                         | Train Hours | Domain                                | Speaking Style         | Casing | Punctuation | Recommended Usage                                 |
|-----------------------------------------------------------------------------------------|-------------|---------------------------------------|------------------------|--------|-------------|---------------------------------------------------|
| [LibriSpeech](https://huggingface.co/datasets/librispeech_asr)                          | 960         | Audiobooks                            | Narrated               | ‚ùå      | ‚ùå           | Academic benchmarks                               |
| [Common Voice 11](https://huggingface.co/datasets/mozilla-foundation/common_voice_11_0) | 2300        | Wikipedia text & crowd-sourced speech | Narrated               | ‚úÖ      | ‚úÖ           | Non-native English speakers                       |
| [TED-LIUM](https://huggingface.co/datasets/LIUM/tedlium)                                | 450         | TED talks                             | Narrated               | ‚ùå      | ‚ùå           | Technical scientific, political and social topics |
| [VoxPopuli](https://huggingface.co/datasets/facebook/voxpopuli)                         | 540         | European Parliament recordings        | Spontaneous            | ‚ùå      | ‚ùå           | Non-native English speakers                       |
| [GigaSpeech](https://huggingface.co/datasets/speechcolab/gigaspeech)                    | 10000       | Audiobook, podcast, youtube           | Narrated & spontaneous | ‚ùå      | ‚úÖ           | Robustness over multiple domains                  |
| [SPGISpeech](https://huggingface.co/datasets/kensho/spgispeech)                         | 5000        | Financial meetings                    | Narrated & spontaneous | ‚úÖ      | ‚úÖ           | Fully formatted transcriptions                    |
| [AMI](https://huggingface.co/datasets/edinburghcstr/ami)                                | 100         | Meetings                              | Spontaneous            | ‚úÖ      | ‚úÖ           | Noisy speech conditions                           |

Summary of multilingual datasets: XYZ

Alright! Now that we've gone through all the criterion for selecting an ASR dataset, let's pick one for the purpose of this tutorial. Let's say we've just developed a fancy new speech recognition system and we want to compare it to current state-of-the-art. Our speech is going to be narrated (scripted) and we‚Äôre not worried about punctuation or casing on our text output. From our reference table, it looks like LibriSpeech is the perfect candidate dataset!

## How to load an audio dataset
Before we can handle any audio datasets, we need to make sure we have the right dependencies installed. Specifically, we'll need the [`audio` feature](https://huggingface.co/docs/datasets/installation#audio) from ü§ó Datasets:

```bash
pip install datasets[audio]
````

Installing the `audio` feature will take care of the main Python packages we need to read audio files and convert them to arrays.

Right! Now we're ready to go ahead and download our data. LibriSpeech is one of the most popular datasets for benchmarking speech recognition systems in both academia and industry. It consists of approximately 1000 hours of narrated audiobooks collected from the [LibriVox](https://librivox.org/) project.

Let‚Äôs take a look at the all-time leaderboard on Papers with Code to get a feel for numbers: https://paperswithcode.com/sota/speech-recognition-on-librispeech-test-clean
We can see that there‚Äôs been tremendous progress on the LibriSpeech benchmark in the last three years, with many of the top spots being occupied by Wav2Vec2 or Wav2Vec2-like systems! As mentioned in Section 4, Wav2Vec2 revolutionised the field of speech recognition by introducing an effective pre-training regime, much the same way as BERT revolutionised NLP. We can see that the word error rates (WERs) for this dataset are extremely low! We‚Äôll cover WER in more detail in this section, but for now we can see that the best performing system achieves a WER of just 1.4%, or a word accuracy rate of 100 - 1.4 = 98.6%! This is very impressive! We‚Äôll see how close we can get with our system ü§ó

Now that we've got better idea about the dataset that we're working with, let's go ahead and download it using the `load_dataset()` function. Audio datasets are large! For 960h of training data and 40h of validation/test data, we can expect to download ~350GB of raw data. This makes working with audio datasets highly impractical, as 350GB is on the upper-limit of what we can fit on most hard drives! To circumvent this issue, ü§ó Datasets implemented a nifty feature called [_streaming_](https://huggingface.co/docs/datasets/stream). Streaming allows us to download chunks of the dataset at a time. Each chunk represents a small subset of the entire dataset. We can use these chunks to iteratively train our model: we download a chunk, train our model on it, and then delete it from our device when we're done. We can then download the next chunk and repeat! This way, we only ever download data as and when we need it. Pretty neat, right! We can enable streaming by passing the argument `streaming=True` to the `load_dataset` function.

LibriSpeech has three training splits: `train.clean.100`, `train.clean.360` and `train.other.500`. We'll combine these to form one 'super' split of 960h of training data:

```python
from datasets import load_dataset, interleave_datasets, IterableDatasetDict

raw_datasets = load_dataset("librispeech_asr", "all", streaming=True)

librispeech = IterableDatasetDict()

librispeech["train"] = interleave_datasets([raw_datasets["train.clean.100"], raw_datasets["train.clean.360"], raw_datasets["train.other.500"]])
librispeech["validation"] = raw_datasets["validation.clean"]
librispeech["test"] = raw_datasets["test.clean"]
```

Let's inspect the dataset:
```python
librispeech
```

**Print Output:**
```
{'train': <datasets.iterable_dataset.IterableDataset at 0x7f66e94afa30>,
 'validation': <datasets.iterable_dataset.IterableDataset at 0x7f66e94b3880>,
 'test': <datasets.iterable_dataset.IterableDataset at 0x7f66e94db670>}
```
Great! We can see that we've set our training, validation and test splits. We don't have any actual data downloaded yet, but the dataset is primed ready to do this!

Let‚Äôs take a look at the first example of the train split. Running the following cell will automatically download the first chunk of data in the LibriSpeech corpus:
```python
next(iter(librispeech["train"]))
```

**Print Output:**
```
{'file': '374-180298-0000.flac',
 'audio': {'path': '374-180298-0000.flac',
  'array': array([ 7.01904297e-04,  7.32421875e-04,  7.32421875e-04, ...,
         -2.74658203e-04, -1.83105469e-04, -3.05175781e-05]),
  'sampling_rate': 16000},
 'text': 'CHAPTER SIXTEEN I MIGHT HAVE TOLD YOU OF THE BEGINNING OF THIS LIAISON IN A FEW LINES BUT I WANTED YOU TO SEE EVERY STEP BY WHICH WE CAME I TO AGREE TO WHATEVER MARGUERITE WISHED',
 'speaker_id': 374,
 'chapter_id': 180298,
 'id': '374-180298-0000'}
```
Alright! We can see that we've got a bunch of information about the dataset. The two data columns `audio` and `text` contain the most important information for our task: the raw audio inputs and the target text outputs.

We can see that we have our target text sample ready - it looks very much like it's taken from an audiobook! It even starts with the chapter number! But what on earth is going on with the audio? ü§î

As it turns out, we represent audio data digitally as a 1-dimensional array. Each array value denotes the amplitude of our audio signal at a particular time step. From the amplitude information, we can reconstruct the frequency spectrum of the audio and recover all the acoustic features.

Since our audio input is continuous, we can't represent an amplitude value for every possible time step. Instead, we _sample_ amplitude values at fixed time steps.

The interval with which we sample our audio is known as the _sampling rate_. For our dataset, we can see that the sampling rate is 16000, meaning 16000 amplitude values are provided each second. Keep the sampling rate in the back of your mind, it'll be important when we come to processing our data later on!

Whilst 1-dimensional arrays are a suitable format for machines, they're not much use to us as humans! We want to be able to **listen** to our audio to get a feel for the speech and recording conditions. Here, we have two options:

1. Convert our 1-dimensional array into a human-friendly format (mp3 or wav).
2. Look to the Hugging Face Hub!

Option 2 seems much easier to me! Let's head over to the LibriSpeech ASR dataset card on the Hugging Face Hub: https://huggingface.co/datasets/librispeech_asr

Right in the middle of the dataset card we have exactly what we're looking for: the dataset viewer! The dataset viewer shows us the first 100 samples of any dataset. What's more, it's loaded up the audio samples ready for us to listen to in real-time! If we hit the play button on the first sample, we can listen to the audio and see the corresponding text. Have a scroll through the samples for the train and test sets to get a better feel for the audio data that we're dealing with. You‚Äôll notice how clear the audio is and how well spoken the sentences are. This provides us with a clue as to why the top models do so well on LibriSpeech! The audio conditions are very conducive to high system performance - with little background noise and speaker variation, modern speech recognition systems have close to solved the LibriSpeech dataset.

## Models for speech recognition

We can decompose speech recognition models into two parts:
1. Encoder: an acoustic model that maps the raw audio input into a sequence of hidden-states.

diagram

2. Decoder: maps the sequence of hidden-states to logits over the vocabulary.

diagram

The encoder is typically of Wav2Vec2 architecture, introduced in Section 4 of this Chapter. We have some flexibility in our choice of decoder. We could either use a simple linear layer that maps the Wav2Vec2 hidden states directly to output logits over our vocabulary. Or, we could pair our Wav2Vec2 encoder with a decoder model, giving a speech encoder-decoder style model, analogous to the NLP encoder-decoder model introduced in Chapter 1.

A simple linear layer will give a smaller, faster overall model, but will be more susceptible to spelling and grammatical errors. Adding a decoder model greatly improves the quality of transcriptions, at the cost of a larger, slower model. This is because the decoder model is pre-trained on a large corpus of text, enabling us to leverage its learned text representations.
Since we want our system to be robust to spelling and grammar, let‚Äôs go ahead and define a speech encoder-decoder style model.

We‚Äôll pair a pre-trained Wav2Vec2 encoder with a pre-trained Distil-BART decoder, yielding a Wav2Vev2-2-BART model:
```python
from transformers import SpeechEncoderDecoderModel

encoder_id = "facebook/wav2vec2-base"
decoder_id = "sshleifer/distilbart-cnn-6-6"

model = SpeechEncoderDecoderModel.from_encoder_decoder_pretrained(encoder_id, decoder_id, encoder_add_adapter=True)

# set special tokens for generation
model.config.decoder_start_token_id = model.decoder.config.bos_token_id
model.config.pad_token_id = model.decoder.config.pad_token_id
model.config.eos_token_id = model.decoder.config.eos_token_id
```

The `add_adapter` argument introduces a small convolutional network between the encoder and decoder models. This adapter network helps interface the encoder and decoder by down-sampling the encoder hidden-states to better match the timescale of the decoder. In practice, including this adapter results in superior performance than the encoder and decoder models alone:

diagram

## Preprocessing the data
Great! Now that we've defined our model we can start preparing our data for training. For this, we'll need to define two objects: a feature extractor and a tokenizer.

```python
from transformers import AutoFeatureExtractor, AutoTokenizer, Wav2Vec2Processor
# load feature extractor (for audio inputs) and tokenizer (for text outputs)
feature_extractor = AutoFeatureExtractor.from_pretrained(encoder_id)
tokenizer = AutoTokenizer.from_pretrained(decoder_id)
# combine under one class
processor = Wav2Vec2Processor(feature_extractor, tokenizer)
```

We're already pretty familiar with the tokenizer: it converts the text data into a set of token IDs that can be interpreted by the model. This we‚Äôll use to prepare our target labels. But the feature extractor is new! Simply put, the feature extractor prepares our input audio data.

A defining feature of Wav2Vec2 is that it accepts a float array corresponding to the raw waveform of the speech signal as an input. We mentioned that the audio data is represented as a 1-dimensional array, so it's already in the right format to be read by the model (a set of continuous inputs at discrete time steps).

So, what exactly does the feature extractor do?

Well, the audio data is in the right format, but we've imposed no restrictions on the values it can take. For our model to work optimally, we want to keep all the inputs within the same dynamic range. This is going to make sure we get a similar range of activations and gradients for our samples, helping with stability and convergence during training.

To do this, we _normalise_ our audio data, by rescaling each sample to zero mean and unit variance, a process called _feature scaling_. It's exactly this feature normalisation that our feature extractor performs!

We can take a look at the feature extractor in operation by applying it to our first audio sample. First, let's compute the mean and variance of our raw audio data:

```python
import numpy as np

sample = next(iter(librispeech["train"]))["audio"]

print(f"Mean: {np.mean(sample['array']):.3}, Variance: {np.var(sample['array']):.3}")
```

```
Mean: -2.17e-05, Variance: 0.00379
```
We can see that the mean is close to zero already, but the variance is a long way off! This would cause our model problems, as the dynamic range of the audio data would be very small and difficult to separate. Let's apply the feature extractor and see what the outputs look like:

```python
inputs = feature_extractor(sample["array"], sampling_rate=sample["sampling_rate"])

print(f"inputs keys: {list(inputs.keys())}")

print(f"Mean: {np.mean(inputs['input_values']):.3}, Variance: {np.var(inputs['input_values']):.3}")
```

```
inputs keys: ['input_values', 'attention_mask']
Mean: -2.8e-09, Variance: 1.0
```
Alright! Our feature extractor returns a dictionary of two arrays: `input_values` and `attention_mask`. The `input_values` are the preprocessed audio inputs that we'd pass to the Wav2Vec2 model. The `attention_mask` is used when we process a _batch_ of audio inputs at once.

We can see that the mean value is now very much closer to zero, and the variance bang-on one! This is exactly the form we want our audio samples in prior to feeding them to the Wav2Vec2 encoder.

Note how we've passed the sampling rate of our audio data to our feature extractor. This is good practice, as the feature extractor performs a check under-the-hood to make sure the sampling rate of our audio data matches the sampling rate expected by the model. If the sampling rate of our audio data did not match the sampling rate of our model, we'd need to upsample or downsample the audio data to 16000.


Let's combine our feature extractor and tokenizer into one function to jointly preprocess our audio and text data. Here, it is important to align the target transcriptions to the decoder's vocabulary. As we saw above, LibriSpeech only contains capital letters ("CHAPTER SIXTEEN..."), whereas BART was pretrained mostly on lower-cased text. To align the target text to the BART vocabulary, we lower-case the transcription targets:

```python
def prepare_dataset(batch):
    # load audio sample
    sample = batch["audio"]
    # normalise audio inputs using feature extractor
    inputs = feature_extractor(sample["array"], sampling_rate=sample["sampling_rate"])
    batch["input_values"] = inputs.input_values[0]
    batch["input_length"] = len(batch["input_values"])

    # process targets: lower-case the text to match decoder's vocabulary
    input_str = batch["text"].lower()
    # tokenize
    batch["labels"] = tokenizer(input_str).input_ids
    return batch
```

We can apply the data preparation function to all of our training examples using dataset's `.map` method:
```python
for split, dataset in librispeech.items():
    librispeech[split] = (
        dataset.map(prepare_dataset)
        .remove_columns(list(librispeech[split].features.keys()))
        .with_format("torch")
    )
```

We can also shuffle the samples in our training split across chunks. Here, the argument `buffer_size` sets the number of samples we download in each chunk of data. Between 500 and 2000 is a good value. The `seed` is set for reproducibility:
```python
vectorized_datasets[split] = vectorized_datasets[split].shuffle(
    buffer_size=500,
    seed=0,
)
```

And that's the dataset download done! With streaming mode, we've made sure that we won't run out of disk space during training! Now, the only thing worse than running out of disk space during training is getting an "out of memory (OOM)" on your GPU üò≠. Everything crashes and we have to start training all over again...

So, how can we avoid getting an OOM? One trick is to filter audio samples above a certain length. With Transformer models, memory scales with sequence-length squared, so **doubling** the length of the audio sample **quadruples** the memory used by the model! By filtering the longest audio samples in our dataset, we can ensure our memory usage remains below a certain threshold.

Let's go ahead and filter all samples in our training set longer than 20 seconds. 20 seconds generally a good cut-off length for training speech recognition systems. With a 20 second cut-off, we ensure that we don't throw away too much data, whilst limiting our maximum memory usage:

```python
MAX_INPUT_LENGTH_IN_SECONDS = 20
max_input_length = MAX_INPUT_LENGTH_IN_SECONDS * feature_extractor.sampling_rate

# filter data that is longer than max_input_length
def is_audio_in_length_range(length):
    return length < max_input_length

vectorized_datasets["train"] = vectorized_datasets["train"].filter(
    is_audio_in_length_range,
    input_columns=["input_length"],
)
```


Define the datacollator:
```python
from dataclasses import dataclass

@dataclass
class DataCollatorSpeechSeq2SeqWithPadding:
    processor: Wav2Vec2Processor
    decoder_start_token_id: int

    def __call__(self, features):
        # split inputs and labels since they have to be of different lengths and need
        # different padding methods
        input_features = [{"input_values": feature["input_values"]} for feature in features]
        label_features = [{"input_ids": feature["labels"]} for feature in features]

        # pad the audio inputs to max length
        batch = self.processor.feature_extractor.pad(input_features, return_tensors="pt")

        # pad the token targets to max length
        labels_batch = self.processor.tokenizer.pad(label_features, return_tensors="pt")

        # replace padding with -100 to ignore loss correctly
        labels = labels_batch["input_ids"].masked_fill(labels_batch.attention_mask.ne(1), -100)

        # if bos token is appended in previous tokenization step,
        # cut bos token here as it's appended later
        if (labels[:, 0] == self.decoder_start_token_id).all().cpu().item():
            labels = labels[:, 1:]

        batch["labels"] = labels

        return batch
```

```python
data_collator = DataCollatorSpeechSeq2SeqWithPadding(
        processor=processor, decoder_start_token_id=model.config.decoder_start_token_id
    )
```

## Metrics for speech recognition

If your familiar with the [Levenshtein distance](https://en.wikipedia.org/wiki/Levenshtein_distance) from NLP, the metrics for assessing speech recognition systems will be familiar! Don't worry if you're not, we'll go through the explanations start-to-finish to make sure you know the different metrics and understand what they mean.

When assessing speech recognition systems, we compare the system's predictions to the target text transcriptions, annotating any errors that are present. We categorise these errors into one of three categories:
1. Substitutions (S): where we transcribe the **wrong word** in our prediction ("sit" instead of "sat")
2. Insertions (I): where we add an **extra word** in our prediction
3. Deletions (D): where we **remove a word** in our prediction

These error categories are the same for all speech recognition metrics. What differs is the level at which we compute these errors: we can either compute them on the _word level_ or on the _character level_.

We'll use a running example for each of the metric definitions. Here, we have a _ground truth_ or _reference_ text sequence:

```python
reference = "the cat sat on the mat"
```

And a predicted sequence from the speech recognition system that we're trying to assess:

```python
prediction = "the cat sit on the"
```

We can see that the prediction is pretty close! But some of the words are not quite right. We'll evaluate this prediction against the reference for the three most popular speech recognition metrics and see what sort of numbers we get for each.

### Word Error Rate
The _word error rate (WER)_ metric is the 'de facto' metric for speech recognition. It calculates substitutions, insertions and deletions on the _word level_. This means errors are annotated on a word-by-word basis. Take our example:


| Reference:  | the | cat | sat     | on  | the | mat |
|-------------|-----|-----|---------|-----|-----|-----|
| Prediction: | the | cat | **sit** | on  | the |     |  |
| Label:      | ‚úÖ   | ‚úÖ   | S       | ‚úÖ   | ‚úÖ   | D   |

Here, we have:
* 1 substitution ("sit" instead of "sat")
* 0 insertions
* 1 deletion ("mat" is missing)

This gives 2 errors in total. To get our error rate, we divide the number of errors by the total number of words in our reference (N), which for this example is 6:

\begin{aligned}
WER &= \frac{S + I + D}{N} \\
&= \frac{1 + 0 + 1}{6} \\
&= 0.333
\end{aligned}

Alright! So we have a WER of 0.333, or 33.3%. That's quite a lot higher than the best LibriSpeech systems from the leaderboards, which were more like 1%!

Notice how the word "sit" only has one character that is wrong, but the entire word is marked incorrect! This is a defining feature of the WER: spelling errors are penalised heavily, no matter how minor they are.

The WER is defined such that _lower is better_: a lower WER means there are fewer errors in our prediction, so a perfect speech recognition system would have a WER of zero (no errors). 

Let's see how we can compute the WER using ü§ó Evaluate:
```python
from evaluate import load

wer_metric = load("wer")

wer = wer_metric.compute(references=[reference], predictions=[prediction])

print(wer)
```
**Print Output:**
```
0.3333333333333333
```

<!---
Now, here's something that's quite confusing! What do you think the upper limit of the WER is? You would expect it to be 1 or 100% right? Nuh uh! Since the WER is the ratio of errors to number of words (N), there is no upper limit on the WER! Let's take an example were we predict 10 words and the target has 2 words. If all of our predictions were wrong (10 errors), we'd have a WER of 10 / 2 = 5, or 500%!
--->

### Word Accuracy
We can flip the WER around to give us a metric where _higher is better_. Rather than measuring the word error rate, we can measure the _word accuracy (WAcc)_ of our system:

\begin{equation}
WAcc = 1 - WER \nonumber
\end{equation}

The WAcc is also measured on the word-level, it's just the WER reformulated as an accuracy metric rather than an error metric. The WAcc is very infrequently quoted in the speech literature - we think of our system predictions in terms of word errors, and so prefer error rate metrics that are more associated with these error type annotations.

### Character Error Rate
It seems a bit unfair that we marked the entire word for "sit" wrong when in fact only one letter was incorrect! That's because we were evaluating our system on the word level, thereby annotating errors on a word-by-word basis. The _character error rate (CER)_ assess systems on the _character level_. This means we divide up our words into their individual characters, and annotate errors on a character-by-character basis:

| Reference:  | t   | h   | e   |     | c   | a   | t   |     | s   | a     | t   |     | o   | n   |     | t   | h   | e   |     | m   | a   | t   |
|-------------|-----|-----|-----|-----|-----|-----|-----|-----|-----|-------|-----|-----|-----|-----|-----|-----|-----|-----|-----|-----|-----|-----|
| Prediction: | t   | h   | e   |     | c   | a   | t   |     | s   | **i** | t   |     | o   | n   |     | t   | h   | e   |     |     |     |     |
| Label:      | ‚úÖ   | ‚úÖ   | ‚úÖ   |     | ‚úÖ   | ‚úÖ   | ‚úÖ   |     | ‚úÖ   | S     | ‚úÖ   |     | ‚úÖ   | ‚úÖ   |     | ‚úÖ   | ‚úÖ   | ‚úÖ   |     | D   | D   | D   |

We can see now that for the word "sit", the "s" and "t" are marked as correct! It's only the "i" which is labelled as a substitution error (S). Thus, we reward our system for the partially correct prediction ü§ù

In our example, we have 1 substitution, 0 insertions, and 3 deletions. In total, we have 14 characters. So, our CER is:

\begin{aligned}
CER &= \frac{S + I + D}{N} \\
&= \frac{1 + 0 + 3}{14} \\
&= 0.286
\end{aligned}

Right! We have a CER of 0.286, or 28.6%! Notice how this is lower than our WER - we penalised the spelling error much less.

```python
cer_metric = load("cer")

cer = cer_metric.compute(references=[reference], predictions=[prediction])

print(cer)
```
**Print Output:**
```

```

### Which metric should I use?
In general, the WER is used far more than the CER for assessing speech systems. This is because the WER requires systems to have greater understanding of the context of the predictions. In our example, "sit" is in the wrong tense. A system that understands the relationship between the verb and tense of the sentence would have predicted the correct verb tense of "sat". We want to encourage this level of understanding from our speech systems. So although the WER is harder than the CER, it's also more conducive to the kinds of intelligible systems we want to develop. Therefore, we typically use the WER. I would encourage you to as well!

In our example, we only used one sentence when computing the WER. We would typically use an entire test set consisting of several thousand sentences when evaluating a real system. When evaluting over multiple sentences, we aggregate S, I, D and N across all sentences, and then compute the WER according to the formula defined above.

## Fine-tuning a speech recognition system with the Trainer API

