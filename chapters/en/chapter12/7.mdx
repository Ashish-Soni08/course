# Llama.cpp

In this chapter, we'll dive into Llama.cpp, a powerful C++ implementation for running LLMs efficiently on consumer hardware. We'll explore its features, installation, and how to optimize model inference using this popular framework. 