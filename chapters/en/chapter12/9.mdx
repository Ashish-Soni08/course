# vLLM

In this chapter, we'll explore vLLM, a high-performance framework for LLM inference. We'll cover its architecture, installation, and how to use it to serve models. 