# Fundamentals of LLM Inference

In this section, we'll explore the core concepts behind LLM inference, providing a comprehensive understanding of how these models generate text and the key components involved in the inference process.

## Understanding the Basics

Let's start with the fundamentals. Inference is the process of using a trained LLM to generate human-like text from a given input prompt. At its core, the model leverages learned probabilities from billions of parameters to predict and generate the next token in a sequence, one token at a time. The model is able to predict the next token based on the previous tokens in the sequence. We explored this in [Chapter 2](/course/chapter2/1) of this course.

## The Role of Attention

One of the most crucial mechanisms that makes LLMs work effectively is attention. When predicting the next word, not every word in a sentence is equally important; words like "France" and "capital" in the sentence *"The capital of France is ..."* carry the most meaning. Furthermore, the model is able to attend to different parts of the input sequence when generating tokens.

<img src="https://huggingface.co/datasets/agents-course/course-images/resolve/main/en/unit1/AttentionSceneFinal.gif" alt="Visual Gif of Attention" width="60%">

This process of identifying the most relevant words to predict the next token has proven to be incredibly effective. Although the basic principle of LLMs—predicting the next token—has remained consistent since BERT and GPT-2, there have been significant advancements in scaling neural networks and making the attention mechanism work for longer and longer sequences, at lower and lower costs.

<Tip>
In short, the attention mechanism is the key to LLMs being able to generate text that is both coherent and context-aware. It sets modern LLMs apart from previous generations of language models.
</Tip>

### Context Length and Attention Span

Attention is crucial, but how much attention is possible?

A key concept in LLM operations is the context length, which refers to the maximum number of tokens the LLM can process, and the maximum _attention span_ it has. This determines how much information the model can consider at once when generating responses.

When we talk talk about attention span, we're talking about the maximum number of tokens the model can consider at once when generating a response. This is a crucial concept to understand because transformers are much more effective at understanding concepts described within their input. However, the attention span is limited by a number of factors, including the model's size, the complexity of the input, and the complexity of the output. In an ideal world, we would just pass a huge amount of context to the model and let it generate output. However, this is not always possible due to these limitations.

<Tip>
The attention span is the maximum number of tokens the model can consider at once when generating a response.
</Tip>

### The Art of Prompting

When we pass information to LLMs, we structure our input in a way that guides the generation of the LLM toward the desired output. This is called _prompting_.

Understanding how LLMs process information helps us craft better prompts. Since the model's primary task is to predict the next token by analyzing the importance of each input token, the wording of your input sequence becomes crucial.

<Tip>
Careful design of the prompt makes it easier **to guide the generation of the LLM toward the desired output**.
</Tip>

## The Two-Phase Inference Process

Let's move on to the inference process. 

The inference process in LLMs can be broken down into two distinct phases, each playing a vital role in generating coherent responses. Those phases are the prefill and the decode phase.

Let's explore how these phases work together.

### The Prefill Phase

The prefill phase is where the initial processing of your input prompt occurs. This phase involves:
- Tokenization of the input
- Embedding conversion
- A single forward pass through the model

We explored this in [Chapter 2](/course/chapter2/1) of this course. If you're not familiar with the prefill phase, we recommend you to check it out.

During this phase, the input prompt is first broken into tokens, which are then converted into numerical representations that capture meaning and context. Finally, these embeddings are processed through attention and feed-forward layers to form a rich internal representation.

This phase is computationally intensive and often compute-bound, meaning its speed is limited by processing power. 

### The Decode Phase

Following the prefill phase, the decode phase begins. This is where the actual text generation happens through an autoregressive process. We explored this in [Chapter 2](/course/chapter2/1) of this course. 

Each token generation requires performing the same attention operation, making it memory-bound. This repeated loading is the major bottleneck to inference speed.

The decode phase consists of several key steps:
- Generating one token at a time in an autoregressive manner
- Calculating probability scores for each possible next token
- Applying various decoding strategies to choose the next token
- Continuing this process until an end-of-sequence (EOS) token is generated

Decoding strategies play a crucial role in determining how the next token is chosen based on computed probabilities. The simplest approach is greedy decoding, which directly selects the token with the highest probability. A more sophisticated method is beam search, which evaluates multiple candidate sequences to optimize overall sentence probability. Additionally, techniques such as temperature scaling, top-k, and nucleus sampling (top-p) provide different ways to balance between deterministic outputs and creative generation.

It's ok if many of these terms are new to you. We'll explore them in detail here.

## Sampling Strategies

Building on our understanding of the decode phase, let's explore the various strategies used to control text generation. These sampling strategies help balance between creativity and coherence in the generated text.

### Understanding Token Selection

The graphic below illustrates how tokens are selected, from the initial probabilities to the final token choice:

<img src="/Users/ben/code/course/chapters/en/chapter12/diagrams/1.png" alt="Token Selection Process" />

The process involves several steps:
1. **Raw Logits**: The output of the last layer of the model, representing the probability of each token in the vocabulary.
2. **Temperature**: A parameter controlling the randomness of token selection - higher for more creativity, lower for more focused output.
3. **Top-p (Nucleus) Sampling**: Selects from the smallest set of tokens whose cumulative probability exceeds the specified top_p value.
4. **Top-k Filtering**: Limits token selection to the k most likely next tokens.

### Managing Repetition

In general, language models tend to repeat themselves which leads to less readable and less coherent text. To address this, we can use presence and frequency penalties. There are two types of penalties:

1. **Presence Penalty** - Penalize tokens that appear in the text.
2. **Frequency Penalty** - Penalize tokens based on their frequency.

As the diagram below shows, the penalties are applied by modifying the logits before the temperature scaling step.

<img src="/Users/ben/code/course/chapters/en/chapter12/diagrams/2.png" alt="Token Selection Process" />

If we walk through the process, the presence penalty (0.1) adds a fixed penalty to tokens that have already appeared in the output text, regardless of how often they've appeared. Frequency penalty (0.1) scales the penalty based on how frequently each token has been used. These penalties are applied by modifying the logits before the temperature scaling step. In short, adjusts the model's score for each token, making it less likely to repeat itself.

### Controlling Generation Length

Language models are great at generating text, but they can also generate text that is too long or too short for the context in which it is used. For example, you might want to use a language model to generate a subtitle for a blog post, or to generate the blog post itself. Ideally, we'd like to control the length of the generated text.We can achieve this by using stop sequences and length control.

We can define stop sequences based on specific tokens or sequences. For example, we can stop the generation when we encounter the token "###" or the sequence "\n\n". These are common stop sequences in language models and will force the model to only generate a single section or line of the text.

In the graphic below, we can see the process of stopping the generation when we encounter the stop sequence "\n\n". 

<img src="/Users/ben/code/course/chapters/en/chapter12/diagrams/3.png" alt="Token Selection Process" />

If we walk through the process, the model will generate at least 10 tokens (min_tokens) but no more than 100 tokens (max_tokens). It will stop immediately if it encounters either "###" or two consecutive newlines. Setting ignore_eos=False means it will also stop when it generates an end-of-sequence token. Special tokens (like padding or system tokens) are filtered from the output.

### Selecting sequences with Beam Search

The parameters we've explored so far work on a token level. The model will generate a single token at a time and select the most likely next token. This relies on functions of the language model's logits to select the next token. 

However, we can also use beam search to select sequences of tokens. This is a more sophisticated approach that can generate more coherent and structured output. It works by maintaining a set of candidate sequences and selecting the most promising ones at each step. 

<img src="/Users/ben/code/course/chapters/en/chapter12/diagrams/4.png" alt="Beam Search" />

Beam search maintains multiple candidate sequences during generation, typically exploring the top 5 most promising sequences at each step. 

The length_penalty parameter affects how beam search scores sequences of different lengths.

#### Guidelines for selecting the best sampling strategy

To help you select the appropriate sampling strategy for your use case, here's a suggestion for some parameters:

| Use Case | Parameters | When to Use | Example Parameters |
|----------|------------|-------------|-------------------|
| Creative Writing | Higher temperature, presence/frequency penalties | For story generation, poetry, creative tasks | `temperature=0.9`<br>`top_p=0.95`<br>`top_k=50`<br>`presence_penalty=0.2`<br>`frequency_penalty=0.2` |
| Factual Generation | Lower temperature, no penalties | For Q&A, summarization, factual responses | `temperature=0.3`<br>`top_p=0.85`<br>`top_k=40`<br>`presence_penalty=0.0`<br>`frequency_penalty=0.0` |
| Balanced Generation | Moderate parameters | For general conversation, content generation | `temperature=0.7`<br>`top_p=0.9`<br>`top_k=50`<br>`presence_penalty=0.1`<br>`frequency_penalty=0.1`<br>`max_tokens=100` |

These parameters are gold standards, and will fluctuate based on the model you're using. But they're a good starting point to explore. 

For most applications, start with the balanced parameters and adjust based on your specific needs:
- Increase temperature and penalties for more creative, diverse outputs
- Decrease temperature and remove penalties for more focused, deterministic responses
- Adjust top_k and top_p to control the randomness vs. quality tradeoff

<Tip>
Why not try these parameters out for yourself? You can use the [Hugging Face Playground](https://huggingface.co/playground) to try out different parameters and see how they affect the output.
</Tip>

## Challenges and constraints in LLM Inference

Now that we've explored the inference process, let's consider some of the key challenges when we design our inference pipeline. In the coming sections we'll explore methods and tools that can help us address these challenges.

First, let's look at the important metrics we should consider when we consider challenges in inference.

- **TTFT** (Time To First Token): How long does it take to generate the very first token? This is primarily affected by the prefill stage.
- **TPOT** (Time Per Output Token): How long does it take to generate each subsequent token? This is where the decode stage and optimizations like KV caching are critical.
- **Throughput**: How many requests or tokens can we process per second? This is heavily influenced by available VRAM and the batch size we can use.
- **VRAM Usage**: The amount of GPU memory used. This is a crucial constraint, as it limits the size of the model, the batch size, and the sequence length we can handle.

We should consider these metrics when we design our inference pipeline, and adapt them based on our use case.

Now, let's consider some of the challenges themselves.
### Context Length

As mentioned above, the context length is the maximum number of tokens the LLM can process. It affects both:

- **Input Processing**: How much text the model can consider at once
- **Memory Usage**: Longer contexts require more VRAM, scaling quadratically with length, because the attention mechanism is memory-bound.
- **Processing Speed**: Affects both prefill and decode phases


<div style="max-width: 800px; margin: 20px auto; padding: 20px; font-family: system-ui;">
    <div style="border: 2px solid #ddd; border-radius: 8px; padding: 20px; margin-bottom: 20px;">
        <div style="display: flex; align-items: center; margin-bottom: 15px;">
            <div style="flex: 1; text-align: center; padding: 10px; background: #f0f0f0; border-radius: 4px;">
                Input Text (Raw)
            </div>
            <div style="margin: 0 10px;">→</div>
            <div style="flex: 1; text-align: center; padding: 10px; background: #e1f5fe; border-radius: 4px;">
                Tokenized Input
            </div>
        </div>
        <div style="display: flex; margin-bottom: 15px;">
            <div style="flex: 1; border: 1px solid #ccc; padding: 10px; margin: 5px; background: #e8f5e9; border-radius: 4px; text-align: center;">
                Context Window<br/>(e.g., 4K tokens)
                <div style="display: flex; margin-top: 10px;">
                    <div style="flex: 1; background: #81c784; margin: 2px; height: 20px; border-radius: 2px;"></div>
                    <div style="flex: 1; background: #81c784; margin: 2px; height: 20px; border-radius: 2px;"></div>
                    <div style="flex: 1; background: #81c784; margin: 2px; height: 20px; border-radius: 2px;"></div>
                    <div style="flex: 1; background: #81c784; margin: 2px; height: 20px; border-radius: 2px;"></div>
                </div>
            </div>
        </div>
        <div style="display: flex; justify-content: space-between; text-align: center; font-size: 0.9em; color: #666;">
            <div style="flex: 1;">
                <div style="border: 1px solid #ffcc80; padding: 8px; margin: 5px; background: #fff3e0; border-radius: 4px;">
                    Memory Usage<br/>∝ Length²
                </div>
            </div>
            <div style="flex: 1;">
                <div style="border: 1px solid #90caf9; padding: 8px; margin: 5px; background: #e3f2fd; border-radius: 4px;">
                    Processing Time<br/>∝ Length
                </div>
            </div>
        </div>
    </div>
</div>

Recent advances have pushed context lengths from thousands to millions of tokens, but this comes with computational trade-offs. Models like [Qwen2.5-1M](https://huggingface.co/Qwen/Qwen2.5-1M) have a context length of 1M tokens, but at a cost of 10x the inference time of a smaller model.

### KV Cache[[kv-cache]]

As mentioned above, the attention operation is memory intensive and the longer the context, the more memory it will use. 

Fortunately, there are some optimizations that we can use to speed up the inference process. KV caching is one of the most important optimizations.

The KV cache is a memory-efficient way to store the keys and values of the attention mechanism. It is used to store the keys and values of the attention mechanism so that we don't have to recompute them for each token. 

Let's look at the difference between standard inference and KV caching.

| Feature | Standard Inference | KV Caching |
|---------|-------------------|-------------|
| Computation per Word | The model repeats the same calculations for every word. | The model reuses past calculations for faster results. |
| Memory Usage | Uses less memory at each step, but memory grows with longer texts. | Uses extra memory to store past information, but keeps things efficient. |
| Speed | Gets slower as the text gets longer because it repeats work. | Stays fast even with longer texts by avoiding repeated work. |
| Efficiency | High computational cost and slower response times. | Faster and more efficient since the model remembers past work. |
| Handling Long Texts | Struggles with long texts due to repeated calculations. | Perfect for long texts as it remembers past steps. |

<!-- TODO: add video -->
<Video src="https://cdn-uploads.huggingface.co/production/uploads/6527e89a8808d80ccff88b7a/PWI-EwqizVLInztmiI7Eo.mp4" />

KV caching is an optimization technique that significantly improves the inference speed of autoregressive transformers by storing and reusing previously computed key (K) and value (V) states during text generation. In autoregressive generation, each new token depends on all previous tokens, which means the model would normally need to recompute attention for all tokens at each step. However, since the attention mechanism is causal (a token only attends to previous tokens), we can cache the K and V matrices from previous computations and reuse them for generating the next token. This optimization reduces the matrix multiplication operations from O(n²) to O(n), where n is the sequence length, resulting in dramatic speed improvements. For example, benchmarks show that KV caching can reduce generation time by up to 4-5x (from ~56 seconds to ~12 seconds for generating 1000 tokens) while using only marginally more GPU memory.

<video src="https://cdn-uploads.huggingface.co/production/uploads/6527e89a8808d80ccff88b7a/HnzDhoJdAbJhSassYjzEy.mp4" />

## Conclusion

Understanding LLM inference is essential for harnessing the full potential of these models. By mastering the processes of tokenization, contextualization, and autoregressive decoding—as well as key concepts like attention and decoding strategies—we can ensure that generated text is both coherent and context-aware.

A well-designed prompt further guides the model toward producing the desired output, making prompt engineering a vital part of working with LLMs.

<!-- TODO: add quiz -->