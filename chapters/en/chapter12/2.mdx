# Fundamentals of LLM Inference

In this chapter, we'll explore the core concepts behind LLM inference, providing a comprehensive understanding of how these models generate text and the key components involved in the inference process.

## Understanding the Basics

Let's start with the fundamentals. Inference is the process of using a trained LLM to generate human-like text from a given input prompt. At its core, the model leverages learned probabilities from billions of parameters to predict and generate the next token in a sequence, one token at a time. The model is able to predict the next token based on the previous tokens in the sequence.

## The Role of Attention

One of the most crucial mechanisms that makes LLMs work effectively is attention. When predicting the next word, not every word in a sentence is equally important; words like "France" and "capital" in the sentence *"The capital of France is ..."* carry the most meaning.

<img src="https://huggingface.co/datasets/agents-course/course-images/resolve/main/en/unit1/AttentionSceneFinal.gif" alt="Visual Gif of Attention" width="60%">

This process of identifying the most relevant words to predict the next token has proven to be incredibly effective. Although the basic principle of LLMs—predicting the next token—has remained consistent since BERT and GPT-2, there have been significant advancements in scaling neural networks and making the attention mechanism work for longer and longer sequences.

<Tip>
BERT and GPT-2 are two of the most important models in the history of LLMs. They are the foundation of the attention mechanism that is used in most modern LLMs.
</Tip>

### Context Length and Attention Span

A key concept in LLM operations is the context length, which refers to the maximum number of tokens the LLM can process, and the maximum _attention span_ it has. This determines how much information the model can consider at once when generating responses.

When we talk talk about attention span, we're talking about the maximum number of tokens the model can consider at once when generating a response. This is a crucial concept to understand because transformers are much more effective at understanding concepts that described within their input. However, the attention span is limited by a number of factors, including the model's size, the complexity of the input, and the complexity of the output. In an ideal world, we would just pass the entire input to the model and let it generate the entire output. However, this is not always possible due to these limitations.

### The Art of Prompting

Understanding how LLMs process information helps us craft better prompts. Since the model's primary task is to predict the next token by analyzing the importance of each input token, the wording of your input sequence becomes crucial.

The input sequence you provide an LLM is called _a prompt_. Careful design of the prompt makes it easier **to guide the generation of the LLM toward the desired output**.

## The Two-Phase Inference Process

The inference process in LLMs can be broken down into two distinct phases, each playing a vital role in generating coherent responses. Let's explore how these phases work together.

### The Prefill Phase

The prefill phase is where the initial processing of your input prompt occurs. This phase involves:
- Tokenization of the input
- Embedding conversion
- A single forward pass through the model

This phase is computationally intensive and often compute-bound, meaning its speed is limited by the processing power of the GPU. During this phase, the input prompt is first broken into tokens, which are then converted into numerical representations that capture meaning and context. Finally, these embeddings are processed through attention and feed-forward layers to form a rich internal representation.

### The Decode Phase

Following the prefill phase, the decode phase begins. This is where the actual text generation happens through an autoregressive process. Each token generation requires loading the model again, making it memory-bound. This repeated loading is a major bottleneck.

The decode phase consists of several key steps:
- Generating one token at a time in an autoregressive manner
- Calculating probability scores for each possible next token
- Applying various decoding strategies to choose the next token
- Continuing this process until an end-of-sequence (EOS) token is generated

These decoding strategies play a crucial role in determining how the next token is chosen based on computed probabilities. The simplest approach is greedy decoding, which directly selects the token with the highest probability. A more sophisticated method is beam search, which evaluates multiple candidate sequences to optimize overall sentence probability. Additionally, techniques such as temperature scaling, top-k, and nucleus sampling (top-p) provide different ways to balance between deterministic outputs and creative generation.

## Advanced Sampling Strategies

Building on our understanding of the decode phase, let's explore the various strategies used to control text generation. These sampling strategies help balance between creativity and coherence in the generated text.

### Understanding Token Selection

The graphic below illustrates how tokens are selected, from the initial probabilities to the final choice:

<img src="/Users/ben/code/course/chapters/en/chapter12/diagrams/1.png" alt="Token Selection Process" />

The process involves several steps:
1. **Raw Logits**: The output of the last layer of the model, representing the probability of each token in the vocabulary.
2. **Temperature**: A parameter controlling the randomness of token selection - higher for more creativity, lower for more focused output.
3. **Top-p (Nucleus) Sampling**: Selects from the smallest set of tokens whose cumulative probability exceeds the specified top_p value.
4. **Top-k Filtering**: Limits token selection to the k most likely next tokens.

### Managing Repetition

In general, language models tend to repeat themselves which leads to less readable and less coherent text. To address this, we can use presence and frequency penalties. There are two types of penalties:

1. **Presence Penalty** - Penalize tokens that appear in the text.
2. **Frequency Penalty** - Penalize tokens based on their frequency.

As the diagram below shows, the penalties are applied by modifying the logits before the temperature scaling step.

<img src="/Users/ben/code/course/chapters/en/chapter12/diagrams/2.png" alt="Token Selection Process" />

If we walk through the diagram, the presence penalty (0.1) adds a fixed penalty to tokens that have already appeared in the output text, regardless of how often they've appeared. Frequency penalty (0.1) scales the penalty based on how frequently each token has been used. These penalties are applied by modifying the logits before the temperature scaling step. In short, adjusts the model's score for each token, making it less likely to repeat itself.

### Controlling Generation Length

Language models are great at generating text, but they can also generate text that is too long or too short for the context in which they are used. For example, you might want to use a language model to generate sub titles for a blog post, or to generate the blog post itself. We can use stop sequences and length control to address this.

We can define define stop sequences based on specific tokens or sequences. For example, we can stop the generation when we encounter the token "###" or the sequence "\n\n". These are common stop sequences in language models and will force the model to only generate a single section or line of the text.

In the graphic below, we can see the process of stopping the generation when we encounter the stop sequence "\n\n". 

<img src="/Users/ben/code/course/chapters/en/chapter12/diagrams/3.png" alt="Token Selection Process" />

If we walk through the diagram, the model will generate at least 10 tokens (min_tokens) but no more than 100 tokens (max_tokens). It will stop immediately if it encounters either "###" or two consecutive newlines. Setting ignore_eos=False means it will also stop when it generates an end-of-sequence token. Special tokens (like padding or system tokens) are filtered from the output.

### Selecting sequences with Beam Search

The examples we've explored so far work on a token level. The model will generate a single token at a time and select the most likely next token. This relies entirely on the language model's logits to produce coherent text. 

However, we can also use beam search to generate more coherent and structured output. It works by maintaining a set of candidate sequences and selecting the most promising ones at each step. 

<img src="/Users/ben/code/course/chapters/en/chapter12/diagrams/4.png" alt="Beam Search" />

Beam search maintains multiple candidate sequences during generation, typically exploring the top 5 most promising sequences at each step. The length_penalty parameter affects how beam search scores sequences of different lengths.

#### Guidelines for selecting the best sampling strategy

To help you select the appropriate sampling strategy for your use case, here's a guide:

| Use Case | Parameters | When to Use | Example Parameters |
|----------|------------|-------------|-------------------|
| Creative Writing | Higher temperature, presence/frequency penalties | For story generation, poetry, creative tasks | `temperature=0.9`<br>`top_p=0.95`<br>`top_k=50`<br>`presence_penalty=0.2`<br>`frequency_penalty=0.2` |
| Factual Generation | Lower temperature, no penalties | For Q&A, summarization, factual responses | `temperature=0.3`<br>`top_p=0.85`<br>`top_k=40`<br>`presence_penalty=0.0`<br>`frequency_penalty=0.0` |
| Balanced Generation | Moderate parameters | For general conversation, content generation | `temperature=0.7`<br>`top_p=0.9`<br>`top_k=50`<br>`presence_penalty=0.1`<br>`frequency_penalty=0.1`<br>`max_tokens=100` |

<Tip>
For most applications, start with the balanced parameters and adjust based on your specific needs:
- Increase temperature and penalties for more creative, diverse outputs
- Decrease temperature and remove penalties for more focused, deterministic responses
- Adjust top_k and top_p to control the randomness vs. quality tradeoff
</Tip>

## Challenges and constraints in LLM Inference

Now that we've explored the inference process, let's consider some of the key challenges when we design our inference pipeline. First, let's look at the important metrics we should consider when we design our inference pipeline.

When we talk about performance, we consider several key metrics:

- **TTFT** (Time To First Token): How long does it take to generate the very first token? This is primarily affected by the prefill stage.
- **TPOT** (Time Per Output Token): How long does it take to generate each subsequent token? This is where the decode stage and optimizations like KV caching are critical.
- **Throughput**: How many requests or tokens can we process per second? This is heavily influenced by available VRAM and the batch size we can use.
- **VRAM Usage**: The amount of GPU memory used. This is a crucial constraint, as it limits the size of the model, the batch size, and the sequence length we can handle.

We should consider these metrics when we design our inference pipeline, and adapt them based on our use case.

Now, let's consider some of the key challenges when we design our inference pipeline and how they impact performance.

### Context Length

The context length is the maximum number of tokens the LLM can process. It affects both:

- **Input Processing**: How much text the model can consider at once
- **Memory Usage**: Longer contexts require more VRAM, scaling quadratically with length
- **Processing Speed**: Affects both prefill and decode phases


<div style="max-width: 800px; margin: 20px auto; padding: 20px; font-family: system-ui;">
    <div style="border: 2px solid #ddd; border-radius: 8px; padding: 20px; margin-bottom: 20px;">
        <div style="display: flex; align-items: center; margin-bottom: 15px;">
            <div style="flex: 1; text-align: center; padding: 10px; background: #f0f0f0; border-radius: 4px;">
                Input Text (Raw)
            </div>
            <div style="margin: 0 10px;">→</div>
            <div style="flex: 1; text-align: center; padding: 10px; background: #e1f5fe; border-radius: 4px;">
                Tokenized Input
            </div>
        </div>
        <div style="display: flex; margin-bottom: 15px;">
            <div style="flex: 1; border: 1px solid #ccc; padding: 10px; margin: 5px; background: #e8f5e9; border-radius: 4px; text-align: center;">
                Context Window<br/>(e.g., 4K tokens)
                <div style="display: flex; margin-top: 10px;">
                    <div style="flex: 1; background: #81c784; margin: 2px; height: 20px; border-radius: 2px;"></div>
                    <div style="flex: 1; background: #81c784; margin: 2px; height: 20px; border-radius: 2px;"></div>
                    <div style="flex: 1; background: #81c784; margin: 2px; height: 20px; border-radius: 2px;"></div>
                    <div style="flex: 1; background: #81c784; margin: 2px; height: 20px; border-radius: 2px;"></div>
                </div>
            </div>
        </div>
        <div style="display: flex; justify-content: space-between; text-align: center; font-size: 0.9em; color: #666;">
            <div style="flex: 1;">
                <div style="border: 1px solid #ffcc80; padding: 8px; margin: 5px; background: #fff3e0; border-radius: 4px;">
                    Memory Usage<br/>∝ Length²
                </div>
            </div>
            <div style="flex: 1;">
                <div style="border: 1px solid #90caf9; padding: 8px; margin: 5px; background: #e3f2fd; border-radius: 4px;">
                    Processing Time<br/>∝ Length
                </div>
            </div>
        </div>
    </div>
</div>

Recent advances have pushed context lengths from thousands to millions of tokens, but this comes with computational trade-offs. Models like Claude 2.1 (200K tokens) and Anthropic's Constitutional AI (100K tokens) demonstrate the ongoing evolution in this space.

### KV Cache[[kv-cache]]

As mentioned above, the attention operation is memory intensive. Fortunately, there are some optimizations that we can use to speed up the inference process. The KV cache is a memory-efficient way to store the keys and values of the attention mechanism. It is a crucial component of the attention mechanism and is used to store the keys and values of the attention mechanism. This is nothing unique to any inference engine, but we'll remind you of it because it's a crucial concept to understand specific optimizations of vLLM.

| Feature | Standard Inference | KV Caching |
|---------|-------------------|-------------|
| Computation per Word | The model repeats the same calculations for every word. | The model reuses past calculations for faster results. |
| Memory Usage | Uses less memory at each step, but memory grows with longer texts. | Uses extra memory to store past information, but keeps things efficient. |
| Speed | Gets slower as the text gets longer because it repeats work. | Stays fast even with longer texts by avoiding repeated work. |
| Efficiency | High computational cost and slower response times. | Faster and more efficient since the model remembers past work. |
| Handling Long Texts | Struggles with long texts due to repeated calculations. | Perfect for long texts as it remembers past steps. |

<!-- TODO: add video -->
<Video src="https://cdn-uploads.huggingface.co/production/uploads/6527e89a8808d80ccff88b7a/PWI-EwqizVLInztmiI7Eo.mp4" />

KV caching is an optimization technique that significantly improves the inference speed of autoregressive transformers by storing and reusing previously computed key (K) and value (V) states during text generation. In autoregressive generation, each new token depends on all previous tokens, which means the model would normally need to recompute attention for all tokens at each step. However, since the attention mechanism is causal (a token only attends to previous tokens), we can cache the K and V matrices from previous computations and reuse them for generating the next token. This optimization reduces the matrix multiplication operations from O(n²) to O(n), where n is the sequence length, resulting in dramatic speed improvements. For example, benchmarks show that KV caching can reduce generation time by up to 4-5x (from ~56 seconds to ~12 seconds for generating 1000 tokens) while using only marginally more GPU memory.

<video src="https://cdn-uploads.huggingface.co/production/uploads/6527e89a8808d80ccff88b7a/HnzDhoJdAbJhSassYjzEy.mp4" />

### Chat Templates

Chat templates are standardized formats that structure conversations with LLMs. They're crucial because:

- They provide consistent formatting across different models
- They help models distinguish between different speakers/roles
- They can include system prompts and metadata

Here's a simple example of a chat template:

```sh
<|system|>You are a helpful AI assistant.<|endoftext|>
<|user|>What is the capital of France?<|endoftext|>
<|assistant|>The capital of France is Paris.<|endoftext|>
```

We won't go into the details of how to create a chat template, but it's good to know that they exist and that they can be very different. If you're interested in learning more, you can check out this [Chat Templates](https://huggingface.co/docs/transformers/en/chat_templating#how-do-i-use-chat-templates) guide.

For this unit, it's important to manage chat templates using tools like transformers, and to be vigilant when changing templates, models, or visa-versa.

## Conclusion

Understanding LLM inference is essential for harnessing the full potential of these models. By mastering the processes of tokenization, contextualization, and autoregressive decoding—as well as key concepts like attention and decoding strategies—we can ensure that generated text is both coherent and context-aware.

A well-designed prompt further guides the model toward producing the desired output, making prompt engineering a vital part of working with LLMs.

<!-- TODO: add quiz -->