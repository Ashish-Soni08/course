# Fundamentals of Inference for LLMs 

Large Language Models (LLMs) have revolutionized how we interact with and generate text. They transform static knowledge into dynamic text generation through a process known as inference. 

In this chapter, we will explore the fundamental concepts and techniques behind LLM inference, providing a comprehensive understanding of how these models generate text.

## What is LLM Inference?

Let's start with the basics. Inference is the process of using a trained LLM to generate human-like text from a given input prompt. At its core, the model leverages learned probabilities from billions of parameters to predict and generate the next token in a sequence, one token at a time. The model is able to predict the next token based on the previous tokens in the sequence.

## Attention is All You Need

A key aspect of the Transformer architecture is **Attention**. When predicting the next word,
not every word in a sentence is equally important; words like "France" and "capital" in the sentence *"The capital of France is ..."* carry the most meaning.

<img src="https://huggingface.co/datasets/agents-course/course-images/resolve/main/en/unit1/AttentionSceneFinal.gif" alt="Visual Gif of Attention" width="60%">

This process of identifying the most relevant words to predict the next token has proven to be incredibly effective.

Although the basic principle of LLMs—predicting the next token—has remained consistent since GPT-2, there have been significant advancements in scaling neural networks and making the attention mechanism work for longer and longer sequences.

If you've interacted with LLMs, you're probably familiar with the term *context length*, which refers to the maximum number of tokens the LLM can process, and the maximum _attention span_ it has.

## Prompting the LLM

Considering that the only job of an LLM is to predict the next token by looking at every input token, and to choose which tokens are "important", the wording of your input sequence is very important.

The input sequence you provide an LLM is called _a prompt_. Careful design of the prompt makes it easier **to guide the generation of the LLM toward the desired output**.

## The Inference Process

The inference process can be broadly divided into two main phases. LLMs operate in an autoregressive manner—each output token is appended to the sequence and used as part of the input for predicting subsequent tokens.

**Prefill Phase:**
- **Tokenization:** Break the input prompt into tokens.
- **Embedding:** Convert tokens into numerical representations that capture meaning and context.
- **Contextualization:** Process these embeddings with attention and feed-forward layers to form a rich internal representation.

**Decode Phase:**
- **Generation:** Produce one token at a time in an autoregressive manner.
- **Probability Computation:** At each step, calculate scores for each token.
- **Token Selection:** Apply decoding strategies (e.g., greedy, beam search, temperature scaling, top-k, nucleus sampling) to choose the next token.
- **Termination:** Continue until an end-of-sequence (EOS) token is generated.

## Considerations in LLM Inference

Highlight how and why inference is challenging.

### Pre-filling

### Attention Mechanism

Attention is the engine behind how an LLM dynamically weighs the importance of different tokens in the input. By computing attention scores, the model identifies which parts of the input are most relevant at each decoding step. For instance, in the phrase "The capital of France is...", words like "France" and "capital" receive higher focus to accurately predict the next token.

### Decoding Strategies

Decoding strategies determine how the next token is chosen based on computed probabilities. Common strategies include:

- **Greedy Decoding**: Directly selecting the token with the highest probability.
- **Beam Search**: Evaluating multiple candidate sequences to optimize overall sentence probability.
- Techniques like **temperature scaling**, **top-k**, and **nucleus sampling (top-p)** also offer a balance between determinism and creativity.

## Chat Templates

- LLMs don't just use language
- chat templates are the interface to language
- 

### Special Tokens

Special tokens are essential directives within an LLM's output. They not only signal the beginning or end of a sequence but also help delineate structured information within the text.

For example:
- GPT-4 uses `<|endoftext|>` to denote the end of text.
- LLaMA3 employs `<|eot_id|>` as an end-of-sequence signal.
- Gemma uses `<end_of_turn>` to indicate the conclusion of a conversation turn.

### Hardware usage

## Conclusion

Understanding LLM inference is essential for harnessing the full potential of these models. By mastering the processes of tokenization, contextualization, and autoregressive decoding—as well as key concepts like attention and decoding strategies—we can ensure that generated text is both coherent and context-aware.

A well-designed prompt further guides the model toward producing the desired output, making prompt engineering a vital part of working with LLMs.

<!-- TODO: add quiz -->