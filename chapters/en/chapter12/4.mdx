# Basic Pipeline Inference

Let's move on to inferring with LLMs for real! In this chapter, we'll explore the `pipeline` abstraction in Transformers, which we have covered in [the previous chapter](chapters/en/chapter2/3.mdx). We learnt that whether we're generating text, analyzing sentiment, or translating languages, pipelines make it incredibly easy to get started. Here we'll look at how to use the pipeline for text generation.

<Tip>

Think of the `pipeline` as a handy utility that handles all the complex machinery behind the scenes. While it's perfect for getting started and prototyping, keep in mind that for production systems, you might want to explore more optimized approaches (which we'll cover in later chapters).

</Tip>

## How Pipelines Work

If you're building a machine learning application - there's usually a lot of setup and technical details to handle. That's where the Transformers pipeline comes in. It automates the process from raw input to human-readable output through three stages:

**1. Preprocessing Stage: Getting Your Data Model-Ready**

The pipeline first prepares your inputs so that the model can understand them. For text, it performs tokenization, breaking down your words and sentences into bite-sized pieces (tokens) that the model can understand. 

**2. Model Inference: The Magic Happens**

During this stage, the pipeline:
- Batches your inputs for efficiency
- Automatically chooses the best available device for computation (CPU or GPU)
- Applies optimizations like half-precision (FP16) inference where it makes sense
- Handles all the technical complexity of running the model

**3. Postprocessing Stage: Output the Results**

Finally, the pipeline transforms the model's raw outputs into something useful for humans:
- Converts token IDs back into readable text
- Transforms model specific outputs (logits) into meaningful probability scores
- Uses probability scores to generate the most likely tokens.

As mentioned before, there's a lot here that pipeline abstracts away for us, so we're not optimizing for production systems, but it's a great way to get started. That said, there are still some things that we can optimize with pipeline. Let's look at some of the key configuration options and how to use them.

## Basic Usage

It's easy to use a pipeline for text generation. Here's a simple example that will get you up and running in no time:

```python
from transformers import pipeline

# Create your AI text generator - it's as simple as ordering a coffee! ‚òï
generator = pipeline(
    "text-generation",                           # What kind of task you want to do
    model="HuggingFaceTB/SmolLM2-1.7B-Instruct", # Which model to use
    torch_dtype="auto",                          # Let the pipeline handle technical details
    device_map="auto"                            # It'll figure out the best device to use
)

# Now let's generate some text - it's this easy!
response = generator(
    "Write a short poem about coding:",          # Your creative prompt
    max_new_tokens=100,                          # How long you want the response to be
    do_sample=True,                              # Add some creative randomness
    temperature=0.7                              # Control the creativity level
)
print(response[0]['generated_text'])
```

## Key Configuration Options

How can we customize the pipeline to suit our needs? 

### Model Loading: Choosing Where to Run

Let's set up our infrastructure for running the pipeline and the model that we'll use.

```python
# Let's run on CPU - perfect for smaller models or when GPU isn't available
generator = pipeline("text-generation", model="HuggingFaceTB/SmolLM2-135M-Instruct", device="cpu")

# Run on GPU (device 0) - great for faster processing!
generator = pipeline("text-generation", model="HuggingFaceTB/SmolLM2-135M-Instruct", device=0)

# Let the pipeline decide what's best - it's smart enough to figure it out!
generator = pipeline(
    "text-generation",
    model="HuggingFaceTB/SmolLM2-1.7B-Instruct",
    device_map="auto",
    torch_dtype="auto"
)
```

In this example we loaded the model twice. Once with the device set to `cpu` and once with the device set to `0` which is the first GPU. The pipeline will automatically choose the best device for the model.

### Generation Parameters: Fine-tuning Your Output

Earlier we covered [the token selection process](chapters/en/chapter12/2.mdx) and how the model uses probabilities to select the next token. Here we look at the parameters that we can use to fine-tune the output with the pipeline.

Think of these parameters as the creative controls for your model's generation. Just like adjusting the settings on a camera for the perfect photo, these options help you get the most out of the model. Like a camera, poor configuration can lead to blurry photos from even the best model.

```python
response = generator(
    "Translate this to French:",
    max_new_tokens=100,     # Set how long the response should be
    do_sample=True,         # Enable creative mode! (instead of always picking the most likely word)
    temperature=0.7,        # Like a creativity dial: higher = more creative, lower = more focused
    top_k=50,               # Only consider the top 50 most likely next words
    top_p=0.95,             # Keep sampling from the top words until we hit 95% probability
    num_return_sequences=1  # How many different versions you want to generate
)
```

### Advanced Optimization: Making Things Faster and More Efficient

Let's move beyond token selection and look at some of the optimizations that we can use to make the pipeline faster.

#### 1. Quantization: Shrinking Models With Reduced Precision
Quantization is like compressing a file - it makes the model smaller and faster while keeping most of its capabilities. We won't cover the details of quantization here, but we'll look at how to use it in the pipeline. In [the next section](chapters/en/chapter12/5.mdx) we'll look at some of the tools that we can use to quantize the model.


```python
from transformers import AutoModelForCausalLM, BitsAndBytesConfig

# Set up our compression settings
quantization_config = BitsAndBytesConfig(
    load_in_4bit=True,              # Use 4-bit precision - great balance of size and quality
    bnb_4bit_compute_dtype="float16", # Use float16 for calculations
    bnb_4bit_quant_type="nf4"      # Use the newer and better nf4 format
)

# Load the model with our optimized settings
model = AutoModelForCausalLM.from_pretrained(
    "HuggingFaceTB/SmolLM2-1.7B-Instruct",
    quantization_config=quantization_config,
    device_map="auto"               # Let it figure out the best device setup
)
```

#### 2. Memory Management: Keeping Your GPU purring

GPUs are high performance devices that perform a lot of operations in parallel. To get the most out of them, we need to manage their memory efficiently. In short, so that they're not sitting around idle waiting for the next batch of data.

```python
import torch

# Clean up GPU memory - like clearing your desk before starting a new task
torch.cuda.empty_cache()

# Check how much memory we're using - always good to keep an eye on this!
torch.cuda.memory_summary()
```

The `torch.cuda.empty_cache()` function is used to clear the cache of the GPU. This is useful when you're running a lot of operations and want to free up memory.

## Processing Multiple Inputs: Batch Processing for Efficiency

One of the coolest features of pipeline is that it handles multiple inputs at once, making everything run faster. Here's how you can process several prompts in one go:

```python
# Let's prepare a bunch of different prompts to process
prompts = [
    "Write a haiku about programming:",
    "Explain what an API is:",
    "Write a short story about a robot:"
]

# Process everything at once - much faster than doing them one by one!
responses = generator(
    prompts,
    batch_size=4,              # Process 4 prompts at a time for speed
    max_new_tokens=100,        # Keep responses reasonably sized
    do_sample=True,            # Add some creative variety
    temperature=0.7            # Balance between creativity and focus
)

# Let's see what we got!
for prompt, response in zip(prompts, responses):
    print(f"üìù Prompt: {prompt}")
    print(f"ü§ñ Response: {response[0]['generated_text']}\n")
```

## Learn More

Want to dive deeper? Here are some great resources to explore:

- [Hugging Face Pipeline Tutorial](https://huggingface.co/docs/transformers/en/pipeline_tutorial) - Your complete guide to pipelines
- [Pipeline API Reference](https://huggingface.co/docs/transformers/en/main_classes/pipelines) - All the technical details
- [Text Generation Parameters](https://huggingface.co/docs/transformers/en/main_classes/text_generation) - Master the art of text generation
- [Model Quantization Guide](https://huggingface.co/docs/transformers/en/perf_infer_gpu_one) - Speed up your models

Remember, the pipeline is just the beginning of your journey with ü§ó Transformers. As you get more comfortable, you'll discover many more powerful features and optimizations to explore!

