# Advanced Inference

Inference is the process of using a model to generate predictions or responses. With generative models like Large Language Models (LLMs), inference is the process of generating text.

While inference might seem straightforward, deploying and using models efficiently requires some consideration of the various factors like performance, cost, and reliability. Large Language Models (LLMs) present unique challenges due to their size and computational requirements.

In this chapter, we'll explore the challenge of inference from multiple perspectives. We'll go from simple pipelines for vibe testing to production-ready solutions for large-scale deployments. We'll also explore inference both via APIs and through local inference, and we'll cover the various frameworks and libraries that can help you deploy your models.

## Contents

1. Fundamentals of Inference for LLMs
2. Basic Pipeline Inference
3. Vision Inference
4. Structured Output Inference for LLMs
5. Ollama
6. Llama.cpp
7. TGI
8. vLLM
9. Inference Providers

## Resources

- [Hugging Face Pipeline Tutorial](https://huggingface.co/docs/transformers/en/pipeline_tutorial)
- [Text Generation Inference Documentation](https://huggingface.co/docs/text-generation-inference/en/index)
- [Pipeline WebServer Guide](https://huggingface.co/docs/transformers/en/pipeline_tutorial#using-pipelines-for-a-webserver)
- [TGI GitHub Repository](https://github.com/huggingface/text-generation-inference)
- [Hugging Face Model Deployment Documentation](https://huggingface.co/docs/inference-endpoints/index)
- [vLLM: High-throughput LLM Serving](https://github.com/vllm-project/vllm)
- [Optimizing Transformer Inference](https://huggingface.co/blog/optimize-transformer-inference)