# Advanced Inference

Inference is the process of using a model to generate an output like text, images, predictions, or other modalities. 

While inference might seem straightforward, deploying and using models efficiently requires some consideration of the various factors like performance, cost, and reliability. Large Language Models (LLMs) present unique challenges due to their size and computational requirements.

In this chapter, we'll explore the challenge of inference from multiple perspectives. We'll go from simple pipelines for vibe testing to production-ready solutions for large-scale deployments. We'll also explore inference both via APIs and through local inference, and we'll cover the various frameworks and libraries that can help you deploy your models.

## Contents

1. Fundamentals of Inference for LLMs
2. APIs vs Local Inference
3. Basic Pipeline Inference
4. Optimized local inference with Ollama/ LMStudio/ llama.cpp
5. Optimized deployment inference with TGI/ vLLM

## Resources

- [Hugging Face Pipeline Tutorial](https://huggingface.co/docs/transformers/en/pipeline_tutorial)
- [Text Generation Inference Documentation](https://huggingface.co/docs/text-generation-inference/en/index)
- [Pipeline WebServer Guide](https://huggingface.co/docs/transformers/en/pipeline_tutorial#using-pipelines-for-a-webserver)
- [TGI GitHub Repository](https://github.com/huggingface/text-generation-inference)
- [Hugging Face Model Deployment Documentation](https://huggingface.co/docs/inference-endpoints/index)
- [vLLM: High-throughput LLM Serving](https://github.com/vllm-project/vllm)
- [Optimizing Transformer Inference](https://huggingface.co/blog/optimize-transformer-inference)