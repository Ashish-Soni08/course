# Optimizing Local Inference

In this chapter, we'll explore various techniques and strategies for optimizing local inference with Large Language Models. We'll cover topics like quantization, caching, and attention optimizations.