# Local Inference Tools

In this chapter, we'll explore three powerful tools for running LLMs locally: Ollama, LMStudio, and llama.cpp. We'll examine their unique features, use cases, and how to effectively leverage each tool for local inference.

## Resources

- [Hugging Face Performance Optimization Guide](https://huggingface.co/docs/transformers/perf_train_gpu_one)
- [PyTorch Memory Management](https://pytorch.org/docs/stable/notes/cuda.html)
- [Flash Attention Paper](https://arxiv.org/abs/2205.14135)
- [Quantization Guide](https://huggingface.co/docs/transformers/main/en/perf_infer_gpu_one)
- [Model Parallelism Tutorial](https://pytorch.org/tutorials/intermediate/model_parallel_tutorial.html)