# TGI and vLLM

In this chapter, we'll explore two powerful frameworks for serving Large Language Models (LLMs) in production: Text Generation Inference (TGI) and vLLM.

## Text Generation Inference (TGI)

Text Generation Inference (TGI) is a toolkit developed by Hugging Face for deploying and serving Large Language Models (LLMs). It's designed to enable high-performance text generation for popular open-source LLMs. TGI is used in production by Hugging Chat - An open-source interface for open-access models.

### Why Use Text Generation Inference?

Text Generation Inference addresses the key challenges of deploying large language models in production. While many frameworks excel at model development, TGI specifically optimizes for production deployment and scaling. Some key features include:

- **Tensor Parallelism**: TGI's can split models across multiple GPUs through tensor parallelism, essential for serving larger models efficiently.
- **Continuous Batching**: The continuous batching system maximizes GPU utilization by dynamically processing requests, while optimizations like Flash Attention and Paged Attention significantly reduce memory usage and increase speed.
- **Token Streaming**: Real-time applications benefit from token streaming via Server-Sent Events, delivering responses with minimal latency.

### How to Use Text Generation Inference

#### Basic Python Usage

TGI uses a simple yet powerful REST API integration which makes it easy to integrate with your applications. 

#### Using the REST API

TGI exposes a RESTful API that accepts JSON payloads. This makes it accessible from any programming language or tool that can make HTTP requests. Here's a basic example using curl:

```bash
# Basic generation request
curl localhost:8080/v1/chat/completions \
    -X POST \
    -d '{
  "model": "tgi",
  "messages": [
    {
      "role": "system",
      "content": "You are a helpful assistant."
    },
    {
      "role": "user",
      "content": "What is deep learning?"
    }
  ],
  "stream": true,
  "max_tokens": 20
}' \
    -H 'Content-Type: application/json'
```

#### Using the `huggingface_hub` Python Client

The `huggingface_hub` python client handles connection management, request formatting, and response parsing:

```python
from huggingface_hub import InferenceClient

client = InferenceClient(
    base_url="http://localhost:8080/v1/",
)

output = client.chat.completions.create(
    model="tgi",
    messages=[
        {"role": "system", "content": "You are a helpful assistant."},
        {"role": "user", "content": "Count to 10"},
    ],
    stream=True,
    max_tokens=1024,
)

for chunk in output:
    print(chunk.choices[0].delta.content)
```

## vLLM

vLLM is a high-performance library for LLM inference and serving. It's designed to maximize throughput while maintaining low latency, making it ideal for production deployments.

### Key Features of vLLM

1. **PagedAttention**
   - Efficient memory management for KV cache
   - Supports longer sequences
   - Reduces memory fragmentation

2. **Continuous Batching**
   - Dynamic request handling
   - Optimized GPU utilization
   - Reduced latency

3. **Tensor Parallelism**
   - Distributed inference across GPUs
   - Efficient model parallelism
   - Scalable deployment

### Using vLLM

#### Basic Python Usage

```python
from vllm import LLM, SamplingParams

# Initialize the model
llm = LLM(model="HuggingFaceTB/SmolLM2-1.7B-Instruct")

# Set sampling parameters
sampling_params = SamplingParams(
    temperature=0.7,
    top_p=0.95,
    max_tokens=100
)

# Generate text
outputs = llm.generate(
    ["Tell me a joke", "What is machine learning?"],
    sampling_params
)

# Process outputs
for output in outputs:
    print(output.outputs[0].text)
```

#### OpenAI-Compatible Server

vLLM provides an OpenAI-compatible server that you can use with existing OpenAI clients:

```python
from vllm.entrypoints.openai.api_server import serve

# Start the server
serve(
    model="HuggingFaceTB/SmolLM2-1.7B-Instruct",
    host="localhost",
    port=8000
)
```

Then use it with any OpenAI client:

```python
from openai import OpenAI

client = OpenAI(base_url="http://localhost:8000/v1", api_key="dummy")

completion = client.chat.completions.create(
    model="HuggingFaceTB/SmolLM2-1.7B-Instruct",
    messages=[
        {"role": "user", "content": "Tell me a joke"}
    ]
)
print(completion.choices[0].message.content)
```

## Comparing TGI and vLLM

Here's a comparison of key aspects:

| Feature | TGI | vLLM |
|---------|-----|------|
| Memory Efficiency | Good (Flash Attention) | Excellent (PagedAttention) |
| Ease of Use | Very Easy | Moderate |
| Deployment Options | Docker, Cloud | Python Package, Docker |
| Model Support | Limited | Broader |
| Integration | HF Ecosystem | OpenAI Compatible |
| Community | Large (HF) | Growing |

## Resources

- [Text Generation Inference Documentation](https://huggingface.co/docs/text-generation-inference)
- [TGI GitHub Repository](https://github.com/huggingface/text-generation-inference)
- [vLLM Documentation](https://vllm.readthedocs.io/)
- [vLLM GitHub Repository](https://github.com/vllm-project/vllm)
- [PagedAttention Paper](https://arxiv.org/abs/2309.06180) 